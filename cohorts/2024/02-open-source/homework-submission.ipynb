{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d50cba-65d2-4670-a2c5-66a198c62ca0",
   "metadata": {},
   "source": [
    "## Q1. Running Ollama with Docker\n",
    "\n",
    "Let's run ollama with Docker. We will need to execute the \n",
    "same command as in the lectures:\n",
    "\n",
    "```bash\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "```\n",
    "\n",
    "What's the version of ollama client? \n",
    "\n",
    "To find out, enter the container and execute `ollama` with the `-v` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2f54be-4dbe-49d7-ac91-76826af8d9ba",
   "metadata": {},
   "source": [
    "```bash\n",
    "@waleedayoub ➜ /workspaces/llm-zoomcamp (main) $ docker exec -it 57e6dc62c677 /bin/bash\n",
    "root@57e6dc62c677:/# ollama -v\n",
    "ollama version is 0.1.48\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfccb298",
   "metadata": {},
   "source": [
    "## Q2. Downloading an LLM \n",
    "\n",
    "We will donwload a smaller LLM - gemma:2b. \n",
    "\n",
    "Again let's enter the container and pull the model:\n",
    "\n",
    "```bash\n",
    "ollama pull gemma:2b\n",
    "```\n",
    "\n",
    "In docker, it saved the results into `/root/.ollama`\n",
    "\n",
    "We're interested in the metadata about this model. You can find\n",
    "it in `models/manifests/registry.ollama.ai/library`\n",
    "\n",
    "What's the content of the file related to gemma?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd527f",
   "metadata": {},
   "source": [
    "- The contents of the file `2b` stores the manifest for the model managed by Ollama\n",
    "- The contents of the manifest is used by Docker to know what to fetch (i.e. `layers` of the manifest) in order to assemble the image\n",
    "\n",
    "```bash\n",
    "root@57e6dc62c677:~/.ollama/models/manifests/registry.ollama.ai/librar\n",
    "y/gemma# cat 2b\n",
    "{\"schemaVersion\":2,\"mediaType\":\"application/vnd.docker.distribution.manifest.v2+json\",\"config\":{\"mediaType\":\"application/vnd.docker.container.image.v1+json\",\"digest\":\"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\"size\":483},\"layers\":[{\"mediaType\":\"application/vnd.ollama.image.model\",\"digest\":\"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\"size\":1678447520},{\"mediaType\":\"application/vnd.ollama.image.license\",\"digest\":\"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\"size\":8433},{\"mediaType\":\"application/vnd.ollama.image.template\",\"digest\":\"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\"size\":136},{\"mediaType\":\"application/vnd.ollama.image.params\",\"digest\":\"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\"size\":84}]}root@57e6dc62c677:~/.ollama/models/manifests/registry.ollama.ai/librar\n",
    "y/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c80656",
   "metadata": {},
   "source": [
    "## Q3. Running the LLM\n",
    "\n",
    "Test the following prompt: \"10 * 10\". What's the answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08803b1",
   "metadata": {},
   "source": [
    "```bash\n",
    "@waleedayoub ➜ /workspaces/llm-zoomcamp (main) $ docker exec -it ollama ollama run gemma:2b\n",
    ">>> 10*10\n",
    "Sure, here is the answer to your question:\n",
    "\n",
    "10 * 10 = 100.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2257066a",
   "metadata": {},
   "source": [
    "## Q4. Downloading the weights \n",
    "\n",
    "We don't want to pull the weights every time we run\n",
    "a docker container. Let's do it once and have them available\n",
    "every time we start a container.\n",
    "\n",
    "First, we will need to change how we run the container.\n",
    "\n",
    "Instead of mapping the `/root/.ollama` folder to a named volume,\n",
    "let's map it to a local directory:\n",
    "\n",
    "```bash\n",
    "mkdir ollama_files\n",
    "\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ./ollama_files:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "```\n",
    "\n",
    "Now pull the model:\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama ollama pull gemma:2b \n",
    "```\n",
    "\n",
    "What's the size of the `ollama_files/models` folder? \n",
    "\n",
    "* 0.6G\n",
    "* 1.2G\n",
    "* 1.7G\n",
    "* 2.2G\n",
    "\n",
    "Hint: on linux, you can use `du -h` for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c454756",
   "metadata": {},
   "source": [
    "- Here is the size of the `./ollama_files/models` folder:\n",
    "```bash\n",
    "waleedayoub ➜ .../llm-zoomcamp/cohorts/2024/02-open-source (main) $ du -h\n",
    "8.0K    ./ollama_files/models/manifests/registry.ollama.ai/library/gemma\n",
    "12K     ./ollama_files/models/manifests/registry.ollama.ai/library\n",
    "16K     ./ollama_files/models/manifests/registry.ollama.ai\n",
    "20K     ./ollama_files/models/manifests\n",
    "1.6G    ./ollama_files/models/blobs\n",
    "1.6G    ./ollama_files/models\n",
    "1.6G    ./ollama_files\n",
    "12K     ./.ipynb_checkpoints\n",
    "1.6G    .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c65af",
   "metadata": {},
   "source": [
    "## Q5. Adding the weights \n",
    "\n",
    "Let's now stop the container and add the weights \n",
    "to a new image\n",
    "\n",
    "For that, let's create a `Dockerfile`:\n",
    "\n",
    "```dockerfile\n",
    "FROM ollama/ollama\n",
    "\n",
    "COPY ...\n",
    "```\n",
    "\n",
    "What do you put after `COPY`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48726fa2",
   "metadata": {},
   "source": [
    "- Here's what the final Dockerfile looks like:\n",
    "```docker\n",
    "FROM ollama/ollama\n",
    "\n",
    "COPY ./ollama_files /root/.ollama\n",
    "```\n",
    "\n",
    "Alternatively you could just copy the models directory over instead of overwriting the ssh keys in the top directory and it'll probably still work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b3cc1",
   "metadata": {},
   "source": [
    "## Q6. Serving it \n",
    "\n",
    "Let's build it:\n",
    "\n",
    "```bash\n",
    "docker build -t ollama-gemma2b .\n",
    "```\n",
    "\n",
    "And run it:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 11434:11434 ollama-gemma2b\n",
    "```\n",
    "\n",
    "We can connect to it using the OpenAI client\n",
    "\n",
    "Let's test it with the following prompt:\n",
    "\n",
    "```python\n",
    "prompt = \"What's the formula for energy?\"\n",
    "```\n",
    "\n",
    "Also, to make results reproducible, set the `temperature` parameter to 0:\n",
    "\n",
    "```bash\n",
    "response = client.chat.completions.create(\n",
    "    #...\n",
    "    temperature=0.0\n",
    ")\n",
    "```\n",
    "\n",
    "How many completion tokens did you get in response?\n",
    "\n",
    "* 304\n",
    "* 604\n",
    "* 904\n",
    "* 1204"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a737baf0",
   "metadata": {},
   "source": [
    "- So the next bit of code is what we'll use to set up the openAI API but point it to the ollama container with the image we built above (which is just a copy of the weights from the gema:2b model)\n",
    "- When calculating tokens, instead of using `tiktoken`, I'm just going to use the `response.usage` method to print out the tokens used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aed5545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6037980",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97023650",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What's the formula for energy?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gemma:2b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "542c96a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's the formula for energy:\n",
      "\n",
      "**E = K + U**\n",
      "\n",
      "Where:\n",
      "\n",
      "* **E** is the energy in joules (J)\n",
      "* **K** is the kinetic energy in joules (J)\n",
      "* **U** is the potential energy in joules (J)\n",
      "\n",
      "**Kinetic energy (K)** is the energy an object possesses when it moves or is in motion. It is calculated as half the product of an object's mass (m) and its velocity (v) squared:\n",
      "\n",
      "**K = 1/2mv^2**\n",
      "\n",
      "**Potential energy (U)** is the energy an object possesses due to its position or configuration. It is calculated as the product of an object's mass, gravitational constant (g), and height or position above a reference point.\n",
      "\n",
      "**U = mgh**\n",
      "\n",
      "Where:\n",
      "\n",
      "* **m** is the mass in kilograms (kg)\n",
      "* **g** is the gravitational constant (9.8 m/s^2)\n",
      "* **h** is the height or position in meters (m)\n",
      "\n",
      "The formula shows that energy can be expressed as the sum of kinetic and potential energy. The kinetic energy is a measure of the object's ability to do work, while the potential energy is a measure of the object's ability to do work against a force.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f45c7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=281, prompt_tokens=34, total_tokens=315)\n"
     ]
    }
   ],
   "source": [
    "print(response.usage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
