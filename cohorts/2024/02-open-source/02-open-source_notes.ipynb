{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Zoomcamp - Week 2 Notes\n",
    "\n",
    "In the second week, we set up cloud-based GPU options like SaturnCloud and explore open source alternatives to OpenAI platforms and  models like:\n",
    "Platforms:\n",
    "- HuggingFace\n",
    "- Ollama\n",
    "- SaturnCloud\n",
    "\n",
    "Models:\n",
    "- Google FLAN T5\n",
    "- Phi 3 Mini\n",
    "- Mistral 7-B\n",
    "\n",
    "And finally, we put the RAG we built in week 1 into a Streamlit UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Using SaturnCloud for GPU Notebooks\n",
    "- The main thing not covered is how to give Saturn Cloud access to your GitHub repositories\n",
    "    - This is fairly straightforward:\n",
    "        1. In Saturn Cloud, go to \"Manage <username>\" and create an SSH key pair\n",
    "        2. Copy the public key Saturn Cloud generates and go to Github.com\n",
    "            i. In Github.com, go to Settings -> SSH and GPG keys and click on `New SSH Key`\n",
    "            ii. Paste in the public key you copied from Saturn Cloud\n",
    "        3. Now go back to Saturn Cloud and click on `Git Repositories`\n",
    "            i. Click on `New`\n",
    "            ii. Add the url for the Github repository you want Saturn Cloud to have access to\n",
    "- When creating a new Python VM resource, make sure to install additional libs: `pip install -U transformers accelerate bitsandbytes`\n",
    "- The rest of it is quite straightforward\n",
    "- A couple of things I did with my setup of the notebook resource that just helps with development:\n",
    "    1. I enabled SSH access so that I can ideally connect to this notebook resource in VS Code (and thus take advantange of many things including Github Copilot)\n",
    "    2. I gave the VM an easy to remember name: https://llm-zoomcamp-waleed.community.saturnenterprise.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 HuggingFace and Google FLAN T5\n",
    "\n",
    "- In this lesson, we start working with open source models available on [HuggingFace](huggingface.co)\n",
    "    - HuggingFace is a place where people host models, not just LLMs, all kinds of ML models (which effectively boils down to hosting model weights)\n",
    "- This is where our Saturn Cloud GPU notebook in 2.2 comes into play as we'll need a GPU to work with these models\n",
    "- We're going to be using Google FLAN T5: https://huggingface.co/google/flan-t5-xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the model on a GPU, here's the code provided in the above link\n",
    "# pip install accelerate\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Import a tokenizer to convert text to tokens\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "# Load the model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")\n",
    "\n",
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An important consideration is how Saturn Cloud provisions storage\n",
    "    - By default, HuggingFace wants to use a /cache subdirectory within the /home directory in your Saturn Cloud environment\n",
    "        - You can change this by setting the `HF_HOME` environment variable\n",
    "        - A better way to do this would be to set it using `direnv` (helpful blog post on that [here](https://waleedayoub.com/post/managing-dev-environments_local-vs-codespaces/#option-2-github-codespaces))\n",
    "    ```python\n",
    "    import os\n",
    "    os.environ['HF_HOME']='/run/cache'\n",
    "    ```\n",
    "    \n",
    "    - The main change we make to our original FAQ answering RAG is the `def llm(query):` function\n",
    "    ```python\n",
    "    def llm(prompt):\n",
    "        input_text = prompt\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model.generate(input_ids)\n",
    "        results = tokenizer.decode(outputs[0])\n",
    "        \n",
    "        return results\n",
    "    ```\n",
    "    - By default, FLAN T5's `generate` method caps the length of the response. You can actually check what the max length is with this:\n",
    "    ```python\n",
    "    print(f\"Default max_length: {model.config.max_length}\")\n",
    "    ```\n",
    "    - This returns: Default max_length: 20\n",
    "    - So this can easily be changed when calling the `generate` method like this:\n",
    "    ```python\n",
    "    outputs = model.generate(input_ids, max_length=200)\n",
    "    ```\n",
    "    - Another useful parameter to the `decode` method is passing `skip_special_tokens` which seems to get rid of the padding leading and trailing tokens\n",
    "    ```python\n",
    "    results = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Phi 3 Mini\n",
    "- Not a lot of notes to take here\n",
    "- We just replaced the FLAN T5 implementation in the previous section with the Microsoft Phi 3 Mini implementation:\n",
    "    - You can find that model here: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Mistral-7B and HuggingFace Hub Authentication"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
