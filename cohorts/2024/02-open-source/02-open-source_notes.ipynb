{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Zoomcamp - Week 2 Notes\n",
    "\n",
    "In the second week, we set up cloud-based GPU options like SaturnCloud and explore open source alternatives to OpenAI platforms and  models like:\n",
    "Platforms:\n",
    "- HuggingFace\n",
    "- Ollama\n",
    "- SaturnCloud\n",
    "\n",
    "Models:\n",
    "- Google FLAN T5\n",
    "- Phi 3 Mini\n",
    "- Mistral 7-B\n",
    "\n",
    "And finally, we put the RAG we built in week 1 into a Streamlit UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Using SaturnCloud for GPU Notebooks\n",
    "- The main thing not covered is how to give Saturn Cloud access to your GitHub repositories\n",
    "    - This is fairly straightforward:\n",
    "        1. In Saturn Cloud, go to \"Manage <username>\" and create an SSH key pair\n",
    "        2. Copy the public key Saturn Cloud generates and go to Github.com\n",
    "            i. In Github.com, go to Settings -> SSH and GPG keys and click on `New SSH Key`\n",
    "            ii. Paste in the public key you copied from Saturn Cloud\n",
    "        3. Now go back to Saturn Cloud and click on `Git Repositories`\n",
    "            i. Click on `New`\n",
    "            ii. Add the url for the Github repository you want Saturn Cloud to have access to\n",
    "- When creating a new Python VM resource, make sure to install additional libs: `pip install -U transformers accelerate bitsandbytes`\n",
    "- The rest of it is quite straightforward\n",
    "- A couple of things I did with my setup of the notebook resource that just helps with development:\n",
    "    1. I enabled SSH access so that I can ideally connect to this notebook resource in VS Code (and thus take advantange of many things including Github Copilot)\n",
    "    2. I gave the VM an easy to remember name: https://llm-zoomcamp-waleed.community.saturnenterprise.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 HuggingFace and Google FLAN T5\n",
    "\n",
    "- In this lesson, we start working with open source models available on [HuggingFace](huggingface.co)\n",
    "    - HuggingFace is a place where people host models, not just LLMs, all kinds of ML models (which effectively boils down to hosting model weights)\n",
    "- This is where our Saturn Cloud GPU notebook in 2.2 comes into play as we'll need a GPU to work with these models\n",
    "- We're going to be using Google FLAN T5: https://huggingface.co/google/flan-t5-xl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by pulling in the minsearch engine we're going to use in our RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T11:10:38.736605Z",
     "iopub.status.busy": "2024-07-07T11:10:38.736233Z",
     "iopub.status.idle": "2024-07-07T11:10:40.052406Z",
     "shell.execute_reply": "2024-07-07T11:10:40.051693Z",
     "shell.execute_reply.started": "2024-07-07T11:10:38.736580Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-07 11:10:39--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3832 (3.7K) [text/plain]\n",
      "Saving to: ‘minsearch.py’\n",
      "\n",
      "minsearch.py        100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-07-07 11:10:39 (51.0 MB/s) - ‘minsearch.py’ saved [3832/3832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -f minsearch.py\n",
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the link above, we have some reference code to run the model on a GPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# pip install accelerate\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Import a tokenizer to convert text to tokens\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "# Load the model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")\n",
    "\n",
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An important consideration is how Saturn Cloud provisions storage\n",
    "    - By default, HuggingFace wants to use a /cache subdirectory within the /home directory in your Saturn Cloud environment\n",
    "        - You can change this by setting the `HF_HOME` environment variable\n",
    "        - A better way to do this would be to set it using `direnv` (helpful blog post on that [here](https://waleedayoub.com/post/managing-dev-environments_local-vs-codespaces/#option-2-github-codespaces))\n",
    "    ```python\n",
    "    import os\n",
    "    os.environ['HF_HOME']='/run/cache'import os\n",
    "    os.environ['HF_HOME']='/run/cache'\n",
    "    ```\n",
    "    - The main change we make to our original FAQ answering RAG is the `def llm(query):` function\n",
    "    ```python\n",
    "    def llm(prompt):\n",
    "        input_text = prompt\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model.generate(input_ids)\n",
    "        results = tokenizer.decode(outputs[0])\n",
    "        \n",
    "        return results\n",
    "    ```\n",
    "    - By default, FLAN T5's `generate` method caps the length of the response. You can actually check what the max length is with this:\n",
    "    ```python\n",
    "    print(f\"Default max_length: {model.config.max_length}\")\n",
    "    ```\n",
    "    - This returns: Default max_length: 20\n",
    "    - So this can easily be changed when calling the `generate` method like this:\n",
    "    ```python\n",
    "    outputs = model.generate(input_ids, max_length=200)\n",
    "    ```\n",
    "    - Another useful parameter to the `decode` method is passing `skip_special_tokens` which seems to get rid of the padding leading and trailing tokens\n",
    "    ```python\n",
    "    results = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    ```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So let's put it all together now and modify our RAG from section 1 to use FLAN T5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T11:17:09.520305Z",
     "iopub.status.busy": "2024-07-07T11:17:09.519947Z",
     "iopub.status.idle": "2024-07-07T11:17:09.524015Z",
     "shell.execute_reply": "2024-07-07T11:17:09.523277Z",
     "shell.execute_reply.started": "2024-07-07T11:17:09.520276Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME']='/run/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T11:17:11.112405Z",
     "iopub.status.busy": "2024-07-07T11:17:11.112048Z",
     "iopub.status.idle": "2024-07-07T11:17:16.376233Z",
     "shell.execute_reply": "2024-07-07T11:17:16.375524Z",
     "shell.execute_reply.started": "2024-07-07T11:17:11.112381Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T11:17:33.342426Z",
     "iopub.status.busy": "2024-07-07T11:17:33.341939Z",
     "iopub.status.idle": "2024-07-07T11:19:56.807187Z",
     "shell.execute_reply": "2024-07-07T11:19:56.806580Z",
     "shell.execute_reply.started": "2024-07-07T11:17:33.342372Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32e87326ff143b5b128ae8c700fed60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f101e8023f0b46598b5696d4cd2d6cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fabe786b5d24d4092f08c7bca7fbd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c08cdd6155743b59da2e6b5c8f332b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca92dd25f244d9c9b55b68d8bc53058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d33e4ba6324bd49e31fa0eefed47f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640e366774d4473c89e91d59efdc30ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbec9e945774079b7f897d1959a20e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15c7571401d4dcda3b4e70c972fdfc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2723f473303d42b6a3178b3987a415c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f58e0d16bc45f1a6e7bbd27bfb2d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import a tokenizer to convert text to tokens\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "# Load the model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T11:19:56.808567Z",
     "iopub.status.busy": "2024-07-07T11:19:56.808295Z",
     "iopub.status.idle": "2024-07-07T11:19:56.813141Z",
     "shell.execute_reply": "2024-07-07T11:19:56.812288Z",
     "shell.execute_reply.started": "2024-07-07T11:19:56.808545Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T11:20:02.675624Z",
     "iopub.status.busy": "2024-07-07T11:20:02.674954Z",
     "iopub.status.idle": "2024-07-07T11:20:02.685473Z",
     "shell.execute_reply": "2024-07-07T11:20:02.684797Z",
     "shell.execute_reply.started": "2024-07-07T11:20:02.675594Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13959,  1566,    12,  2968,    10,   571,   625,    33,    25,    58,\n",
      "             1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T11:20:10.108471Z",
     "iopub.status.busy": "2024-07-07T11:20:10.107795Z",
     "iopub.status.idle": "2024-07-07T11:20:12.058214Z",
     "shell.execute_reply": "2024-07-07T11:20:12.057399Z",
     "shell.execute_reply.started": "2024-07-07T11:20:10.108442Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 2739, 4445,  436,  292,   58,    1]], device='cuda:0')\n",
      "<pad> Wie alt sind Sie?</s>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids)\n",
    "print(outputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we do the same RAG as before but we modify the llm function to use our local FLAN T5 weights vs an OpenAI API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T11:20:29.301812Z",
     "iopub.status.busy": "2024-07-07T11:20:29.301424Z",
     "iopub.status.idle": "2024-07-07T11:20:35.344480Z",
     "shell.execute_reply": "2024-07-07T11:20:35.343669Z",
     "shell.execute_reply.started": "2024-07-07T11:20:29.301786Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7fd4b21d5b50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "import minsearch\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T11:20:37.743414Z",
     "iopub.status.busy": "2024-07-07T11:20:37.742934Z",
     "iopub.status.idle": "2024-07-07T11:20:37.747419Z",
     "shell.execute_reply": "2024-07-07T11:20:37.746770Z",
     "shell.execute_reply.started": "2024-07-07T11:20:37.743389Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T11:20:49.329303Z",
     "iopub.status.busy": "2024-07-07T11:20:49.328927Z",
     "iopub.status.idle": "2024-07-07T11:20:49.334932Z",
     "shell.execute_reply": "2024-07-07T11:20:49.334130Z",
     "shell.execute_reply.started": "2024-07-07T11:20:49.329276Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    input_text = prompt\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids, max_length=200)\n",
    "    results = tokenizer.decode(outputs[0])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T11:20:54.368177Z",
     "iopub.status.busy": "2024-07-07T11:20:54.367786Z",
     "iopub.status.idle": "2024-07-07T11:20:54.372033Z",
     "shell.execute_reply": "2024-07-07T11:20:54.371254Z",
     "shell.execute_reply.started": "2024-07-07T11:20:54.368151Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T11:20:55.807389Z",
     "iopub.status.busy": "2024-07-07T11:20:55.807007Z",
     "iopub.status.idle": "2024-07-07T11:20:57.719593Z",
     "shell.execute_reply": "2024-07-07T11:20:57.718795Z",
     "shell.execute_reply.started": "2024-07-07T11:20:55.807366Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<pad> Yes, even if you don't register, you're still eligible to submit the homeworks. Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.</s>\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"I just discovered the course, can I still register?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T11:21:00.408429Z",
     "iopub.status.busy": "2024-07-07T11:21:00.408065Z",
     "iopub.status.idle": "2024-07-07T11:21:00.412597Z",
     "shell.execute_reply": "2024-07-07T11:21:00.411788Z",
     "shell.execute_reply.started": "2024-07-07T11:21:00.408404Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default max_length: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Default max_length: {model.config.max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Phi 3 Mini\n",
    "- Not a lot of notes to take here\n",
    "- We just replaced the FLAN T5 implementation in the previous section with the Microsoft Phi 3 Mini implementation:\n",
    "    - You can find that model here: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\n",
    "- I'm not going to bother reproducing all the same code for the Phi3 model and will instead focus on the next section using Mistral7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Mistral-7B and HuggingFace Hub Authentication\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
