{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Zoomcamp - Week 5 Notes\n",
    "\n",
    "A quick recap of what the first three sections have been about:\n",
    "- Let's chart with the visual of what we created and map it back to our `rag` function:\n",
    "  ```mermaid\n",
    "    graph TD\n",
    "        A[User] -->|Q| B[Knowledge DB]\n",
    "        B -->|Relevant Documents D1, D2, ..., DN| C[Context = Prompt + Q + Documents]\n",
    "        A -->|Q| C\n",
    "        C -->|Q| D[LLM]\n",
    "        D -->|Answer| A\n",
    "        subgraph Context\n",
    "            direction LR\n",
    "            D1\n",
    "            D2\n",
    "            D3\n",
    "            D4\n",
    "            ...\n",
    "            DN\n",
    "        end\n",
    "        B -.-> D1\n",
    "        B -.-> D2\n",
    "        B -.-> D3\n",
    "        B -.-> D4\n",
    "        B -.-> ...\n",
    "        B -.-> DN\n",
    "        classDef entity fill:#f9f,stroke:#333,stroke-width:4px;\n",
    "    ```\n",
    "  - and the function itself was:\n",
    "    ```python\n",
    "    def rag(query):\n",
    "      search_results = search(query)\n",
    "      prompt = prompt_builder(query, search_results)\n",
    "      answer = llm(prompt)\n",
    "      return answer\n",
    "    ```\n",
    "- In section 1:\n",
    "  - We built the scaffold for the function above\n",
    "  - We learned all about what a RAG is, how to apply a common \"search\" problem using a source document as context, how to implement one using OpenAI's GPT models, and how to use Elasticsearch to do \"semantic\" or \"keyword\" search to simplify the size of the documents being passed to the LLM\n",
    "- In section 2:\n",
    "  - We implemented various versions of the `llm` function\n",
    "  - We focused further on self-hosted LLMs and how to effectively replicate everything we did in section 1 but using `ollama` as a platform to access self-hosted models\n",
    "  - I further set up my windows gaming PC to act as a server running 3 containers: `ollama`, `openwebui` and `elasticsearch` in order to have \"always on\" access to these services\n",
    "- In section 3:\n",
    "  - We experimented with various implementations of the `search` function\n",
    "  - We switched from doing a straight \"semantic\" or \"keyword\" search using Elasticsearch to creating embeddings in order to do vector search. The main difference here is that instead of relying on Elasticsearch's Lucine engine to look up relevant documents based on a text query, we were using various `encoding` algorithms like cosine distance, SBERT models, etc.\n",
    "  - We then built a ground-truth dataset using LLMs in order to evaluate the quality of our retrieval system and compared the performance of \"semantic\" search and \"vector\" search in retrieving the most relevant documents for a given query\n",
    "\n",
    "In this section the focus is on the following:\n",
    "- Extending the evaluation work we did in section 3 to monitor answer quality over time\n",
    "- How to look at answer quality with user feedback and interaction\n",
    "- How to store all this data and visualize it, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Offline RAG Evaluation - Cosine Similarity\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-zoom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
