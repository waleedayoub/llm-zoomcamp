{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91be04e6-dc33-402c-b84d-ad069b3d04b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c6dbd30-4365-40e0-a46c-79e5c6f0b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbc5fcd3-7983-47ad-94a5-2ed38363436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "667f7635-43ab-42cf-a21b-1267ead10a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649695e-8ba6-41fa-915e-fc85566acc1a",
   "metadata": {},
   "source": [
    "# 1.3 Retrieval and Search - Notes\n",
    "- Before jumping into using elasticsearch to index our documents, we're going to use the search engine build by DTC\n",
    "- In order to do that, I need to import minsearch.py (the search engine library)\n",
    "- Once that's done, we need to understand a few things about how `minsearch` is implemented:\n",
    "    - `minsearch.Index()` is the method used to index a document and takes a few parameters: `text_fields` and `keyword_fields`\n",
    "        - text_fields: the fields we use to search\n",
    "        - keyword_fields: the fields used to group the data (i.e. similar to a WHERE clause in SQL)\n",
    "    - So for example, if you pass a query like: ***\"How do I execute a command in a running docker container?\"*** the search engine would filter results by `keyword_fields` and would search through `text_fields`\n",
    "    - index.fit() is the method used to specify the document you want to *fit* your Index to. So in this case, you would pass it the document containing all the data with the relevant keyword_fields and text_fields\n",
    "    - index.search():\n",
    "        - This is the method used to actually search the fitted document for the specific question\n",
    "        - All the `text_fields` you search through are given equal weighting. If you want to change that, you can use a parameter called `boost` which allows you to ***relatively*** overweight or underweight certain `text_fields` by passing it a dictionary with `{text_field: weight}`\n",
    "        - There are two other parameters, that are pretty straightforward: `filter_dict` which just lets you filter based on a `keyword_fields` again as a dict of the form `{\"keyword_fields\": \"value\"}` entry and `num_results` which just limits the number of elements it returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d7492ed-677b-4288-bbe1-2ca1dc925f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the relative path in order to import minsearch.py\n",
    "current_dir = os.getcwd()\n",
    "intro_dir = os.path.abspath(os.path.join(current_dir, \"../../../01-intro\"))\n",
    "sys.path.append(intro_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7711776d-d8e4-47fc-9d97-a0dd757b2f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abd9b7c5-b824-4426-b0df-6f4113ce2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the FAQ documents (already parsed into json) into a list called documents\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af4f7291-a700-46b5-bd02-ef0bb672b8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Field names: ['section', 'text', 'course', 'question']\n",
      "\n",
      "courses:\n",
      " data-engineering-zoomcamp\n",
      "mlops-zoomcamp\n",
      "machine-learning-zoomcamp\n"
     ]
    }
   ],
   "source": [
    "field_names = {key for document in documents for key in document.keys()}\n",
    "print(\"\\nField names:\", list(field_names));\n",
    "print(\"\\ncourses:\\n\",'\\n'.join({course['course'] for course in documents_raw}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1e8b208-1ee2-4304-b3c6-fef5b45da36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index based on the fields in our FAQ document\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbc06440-7adf-4505-b627-7347ca563416",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I execute a command in a running docker container?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4363bacf-96c8-4cc0-a5f7-30b7cce31b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7beed7782170>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9161d828-1332-4e94-b26f-f58bb36dce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = {\n",
    "    \"question\":3,\n",
    "    \"text\":1,\n",
    "    \"section\":0.5\n",
    "}\n",
    "\n",
    "results = index.search(\n",
    "    query = question,\n",
    "    filter_dict = {\"course\":\"data-engineering-zoomcamp\"},\n",
    "    boost_dict = boost,\n",
    "    num_results = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf43f0c9-e23d-420f-8ead-28bb76863c2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'In case running pgcli  locally causes issues or you do not want to install it locally you can use it running in a Docker container instead.\\nBelow the usage with values used in the videos of the course for:\\nnetwork name (docker network)\\npostgres related variables for pgcli\\nHostname\\nUsername\\nPort\\nDatabase name\\n$ docker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\\n175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\\nPassword for root:\\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\\nVersion: 4.0.1\\nHome: http://pgcli.com\\nroot@pg-database:ny_taxi> \\\\dt\\n+--------+------------------+-------+-------+\\n| Schema | Name             | Type  | Owner |\\n|--------+------------------+-------+-------|\\n| public | yellow_taxi_data | table | root  |\\n+--------+------------------+-------+-------+\\nSELECT 1\\nTime: 0.009s\\nroot@pg-database:ny_taxi>',\n",
       " 'section': 'Module 1: Docker and Terraform',\n",
       " 'question': 'PGCLI - running in a Docker container',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c6171-1c68-458c-b72e-d448f3046c99",
   "metadata": {},
   "source": [
    "# 1.4 Generating Answers with OpenAI GPT 4.0\n",
    "- In this section, we'll be packaging up the response from our basic search engine in 1.3 and passing it as part of the context to the OpenAI API\n",
    "- Using the completions API is pretty straightforward for basic usage.\n",
    "    - The documentation for the compeletions API can be found here: https://platform.openai.com/docs/api-reference/chat/create\n",
    "- The general structure of this section is as follows:\n",
    "    - Assume a set of results are generated based on the minsearch (or any search engine) in the previous section\n",
    "    - Based on how it's implemented, `index.search()` returns a list containing entries for each result it returns\n",
    "        - Each result is stored as a dictionary with key-value pairs consisting of {'text', 'section', 'question','course'}\n",
    "    - We want to build a context that includes instructions to the LLM to restrict its answers to content from the results above *AND* the relevant content from those results for it to analyze\n",
    "    - We pass that context as a prompt to the LLM and get results back\n",
    "    - That's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e30eae7a-4310-4240-8997-8202b875f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a teaching assistant for a bootcamp course.\n",
    "Restrict your answers to the QUESTION to the content in CONTEXT only.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: {context}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e65796e-e630-4bb9-af16-d2cc73cf652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\n",
    "\n",
    "for doc in results:\n",
    "    context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f2f27e6-ebc3-4cdf-8aff-bcf4727c40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(question=question, context=context).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f617219-33fe-4e69-8fdb-de33d684179f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a teaching assistant for a bootcamp course.\n",
      "Restrict your answers to the QUESTION to the content in CONTEXT only.\n",
      "\n",
      "QUESTION: How do I execute a command in a running docker container?\n",
      "\n",
      "CONTEXT: section: Module 1: Docker and Terraform\n",
      "question: PGCLI - running in a Docker container\n",
      "answer: In case running pgcli  locally causes issues or you do not want to install it locally you can use it running in a Docker container instead.\n",
      "Below the usage with values used in the videos of the course for:\n",
      "network name (docker network)\n",
      "postgres related variables for pgcli\n",
      "Hostname\n",
      "Username\n",
      "Port\n",
      "Database name\n",
      "$ docker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\n",
      "175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\n",
      "Password for root:\n",
      "Server: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\n",
      "Version: 4.0.1\n",
      "Home: http://pgcli.com\n",
      "root@pg-database:ny_taxi> \\dt\n",
      "+--------+------------------+-------+-------+\n",
      "| Schema | Name             | Type  | Owner |\n",
      "|--------+------------------+-------+-------|\n",
      "| public | yellow_taxi_data | table | root  |\n",
      "+--------+------------------+-------+-------+\n",
      "SELECT 1\n",
      "Time: 0.009s\n",
      "root@pg-database:ny_taxi>\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - Error response from daemon: Conflict. The container name \"pg-database\" is already in use by container “xxx”.  You have to remove (or rename) that container to be able to reuse that name.\n",
      "answer: Sometimes, when you try to restart a docker image configured with a network name, the above message appears. In this case, use the following command with the appropriate container name:\n",
      ">>> If the container is running state, use docker stop <container_name>\n",
      ">>> then, docker rm pg-database\n",
      "Or use docker start instead of docker run in order to restart the docker image without removing it.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - Cannot pip install on Docker container (Windows)\n",
      "answer: You may have this error:\n",
      "Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\n",
      "rllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\n",
      "/simple/pandas/\n",
      "Possible solution might be:\n",
      "$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - Cannot connect to Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n",
      "answer: Make sure you're able to start the Docker daemon, and check the issue immediately down below:\n",
      "And don’t forget to update the wsl in powershell the  command is wsl –update\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: How do I check compatibility of local and container Spark versions?\n",
      "answer: You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - docker pull dbpage\n",
      "answer: Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage).\n",
      "IF the repository is public, the fetch and download happens without any issue whatsoever.\n",
      "For instance:\n",
      "docker pull postgres:13\n",
      "docker pull dpage/pgadmin4\n",
      "BE ADVISED:\n",
      "\n",
      "The Docker Images we'll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\n",
      "\n",
      "Meaning: you are NOT required to perform a docker login to fetch them. \n",
      "\n",
      "So if you get the message above saying \"docker login': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\n",
      "\n",
      "For instance:\n",
      "$ docker pull dbpage/pgadmin4\n",
      "Will throw that exception telling you \"repository does not exist or may require 'docker login'\n",
      "Error response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \n",
      "may require 'docker login': denied: requested access to the resource is denied\n",
      "But that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\n",
      "How to fix it:\n",
      "$ docker pull dpage/pgadmin4\n",
      "EXTRA NOTES:\n",
      "In the real world, occasionally, when you're working for a company or closed organisation, the Docker image you're trying to fetch might be under a private repo that your DockerHub Username was granted access to.\n",
      "For which cases, you must first execute:\n",
      "$ docker login\n",
      "Fill in the details of your username and password.\n",
      "And only then perform the `docker pull` against that private repository\n",
      "Why am I encountering a \"permission denied\" error when creating a PostgreSQL Docker container for the New York Taxi Database with a mounted volume on macOS M1?\n",
      "Issue Description:\n",
      "When attempting to run a Docker command similar to the one below:\n",
      "docker run -it \\\n",
      "-e POSTGRES_USER=\"root\" \\\n",
      "-e POSTGRES_PASSWORD=\"root\" \\\n",
      "-e POSTGRES_DB=\"ny_taxi\" \\\n",
      "-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n",
      "-p 5432:5432 \\mount\n",
      "postgres:13\n",
      "You encounter the error message:\n",
      "docker: Error response from daemon: error while creating mount source path '/path/to/ny_taxi_postgres_data': chown /path/to/ny_taxi_postgres_data: permission denied.\n",
      "Solution:\n",
      "1- Stop Rancher Desktop:\n",
      "If you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\n",
      "2- Install Docker Desktop:\n",
      "Install Docker Desktop, ensuring that it is properly configured and has the required permissions.\n",
      "2-Retry Docker Command:\n",
      "Run the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\n",
      "Note: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.\n",
      "\n",
      "section: Workshop 2 - RisingWave\n",
      "question: Setup - source command.sh - error: “docker-compose” not found\n",
      "answer: If you encounter this error and are certain that you have docker compose installed, but typically run it as docker compose without the hyphen, then consider editing command.sh file by removing the hyphen from ‘docker-compose’. Example:\n",
      "start-cluster() {\n",
      "docker compose -f docker/docker-compose.yml up -d\n",
      "}\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker-Compose - Error getting credentials after running docker-compose up -d\n",
      "answer: Installing pass via ‘sudo apt install pass’ helped to solve the issue. More about this can be found here: https://github.com/moby/buildkit/issues/1078\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - ERRO[0000] error waiting for container: context canceled\n",
      "answer: You might have installed docker via snap. Run “sudo snap status docker” to verify.\n",
      "If you have “error: unknown command \"status\", see 'snap help'.” as a response than deinstall docker and install via the official website\n",
      "Bind for 0.0.0.0:5432 failed: port is a\n",
      "\n",
      "section: Module 2: Workflow Orchestration\n",
      "question: Docker: container crashed with status code 137.\n",
      "answer: It means your container consumed all available RAM allocated to it. It can happen in particular when working on Question#3 in the homework as the dataset is relatively large and containers eat a lot of memory in general.\n",
      "I would recommend restarting your computer and only starting the necessary processes to run the container. If that doesn’t work, allocate more resources to docker. If also that doesn’t work because your workstation is a potato, you can use an online compute environment service like GitPod, which is free under under 50 hours / month of use.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - Docker network name\n",
      "answer: Get the network name via: $ docker network ls.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: PGCLI - INKhould we run pgcli inside another docker container?\n",
      "answer: In this section of the course, the 5432 port of pgsql is mapped to your computer’s 5432 port. Which means you can access the postgres database via pgcli directly from your computer.\n",
      "So No, you don’t need to run it inside another container. Your local system will do.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: GCP - Do I need to delete my instance in Google Cloud?\n",
      "answer: In this lecture, Alexey deleted his instance in Google Cloud. Do I have to do it?\n",
      "Nope. Do not delete your instance in Google Cloud platform. Otherwise, you have to do this twice for the week 1 readings.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - Docker won't start or is stuck in settings (Windows 10 / 11)\n",
      "answer: First off, make sure you're running the latest version of Docker for Windows, which you can download from here. Sometimes using the menu to \"Upgrade\" doesn't work (which is another clear indicator for you to uninstall, and reinstall with the latest version)\n",
      "If docker is stuck on starting, first try to switch containers by right clicking the docker symbol from the running programs and switch the containers from windows to linux or vice versa\n",
      "[Windows 10 / 11 Pro Edition] The Pro Edition of Windows can run Docker either by using Hyper-V or WSL2 as its backend (Docker Engine)\n",
      "In order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\n",
      "If you opt-in for WSL2, you can follow the same steps as detailed in the tutorial here\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - The input device is not a TTY (Docker run for Windows)\n",
      "answer: You may have this error:\n",
      "$ docker run -it ubuntu bash\n",
      "the input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty'\n",
      "error:\n",
      "Solution:\n",
      "Use winpty before docker command (source)\n",
      "$ winpty docker run -it ubuntu bash\n",
      "You also can make an alias:\n",
      "echo \"alias docker='winpty docker'\" >> ~/.bashrc\n",
      "OR\n",
      "echo \"alias docker='winpty docker'\" >> ~/.bash_profile\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - Connecting from VS Code\n",
      "answer: It’s very easy to manage your docker container, images, network and compose projects from VS Code.\n",
      "Just install the official extension and launch it from the left side icon.\n",
      "It will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.\n",
      "Docker - How to stop a container?\n",
      "Use the following command:\n",
      "$ docker stop <container_id>\n",
      "\n",
      "section: Workshop 2 - RisingWave\n",
      "question: command.sh Error - source: no such file or directory: command.sh\n",
      "answer: Check the contents of the repository with ls - the command.sh file should be in the root folder\n",
      "If it is not, verify that you had cloned the correct repository - https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - Cannot install docker on MacOS/Windows 11 VM running on top of Linux (due to Nested virtualization).\n",
      "answer: terraformRun this command before starting your VM:\n",
      "On Intel CPU:\n",
      "modprobe -r kvm_intel\n",
      "modprobe kvm_intel nested=1\n",
      "On AMD CPU:\n",
      "modprobe -r kvm_amd\n",
      "modprobe kvm_amd nested=1\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker-Compose - docker-compose still not available after changing .bashrc\n",
      "answer: This is happen to me after following 1.4.1 video where we are installing docker compose in our Google Cloud VM. In my case, the docker-compose file downloaded from github named docker-compose-linux-x86_64 while it is more convenient to use docker-compose command instead. So just change the docker-compose-linux-x86_64 into docker-compose.\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "944bd3c5-c2fe-4a80-8ada-a7a6be1c2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4o\",\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7e669b5-2cf2-4f8c-b7df-f0299996b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To execute a command in a running Docker container, you can use the `docker exec` command. \n",
      "\n",
      "For example, to execute the command `pgcli -h pg-database -U root -p 5432 -d ny_taxi` inside a running Docker container with the name or ID `175dd47cda07`, you can use:\n",
      "\n",
      "```sh\n",
      "docker exec -it 175dd47cda07 pgcli -h pg-database -U root -p 5432 -d ny_taxi\n",
      "```\n",
      "\n",
      "Here:\n",
      "\n",
      "- `docker exec` is the command to run a command in a running container.\n",
      "- `-it` allows you to interact with the container.\n",
      "- `175dd47cda07` is the name or ID of the container in which you want to run the command.\n",
      "- `pgcli -h pg-database -U root -p 5432 -d ny_taxi` is the command you want to execute inside the container.\n",
      "\n",
      "Make sure to replace `175dd47cda07` with your actual container ID or name.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab2d27-f3d2-44ad-81b4-a83ad2bb1060",
   "metadata": {},
   "source": [
    "# 1.5 The RAG Flow Cleaning and Modularizing Code\n",
    "- At this point, here is the general RAG flow we've built\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[User] -->|Q| B[Knowledge DB]\n",
    "    B -->|Relevant Documents D1, D2, ..., DN| C[Context = Prompt + Q + Documents]\n",
    "    A -->|Q| C\n",
    "    C -->|Q| D[LLM]\n",
    "    D -->|Answer| A\n",
    "    subgraph Context\n",
    "        direction LR\n",
    "        D1\n",
    "        D2\n",
    "        D3\n",
    "        D4\n",
    "        ...\n",
    "        DN\n",
    "    end\n",
    "    B -.-> D1\n",
    "    B -.-> D2\n",
    "    B -.-> D3\n",
    "    B -.-> D4\n",
    "    B -.-> ...\n",
    "    B -.-> DN\n",
    "    classDef entity fill:#f9f,stroke:#333,stroke-width:4px;\n",
    "```\n",
    "- So now we need to clean everything up and put it into useable functions\n",
    "- The way we're going to do that is create x functions:\n",
    "    - `search(query)`: this function just takes in the question and returns the results of type list from the knowledge-base we've implemented\n",
    "    - `build_prompt`: this builds the prompt based on the user prompt + the question + the results from the knowledge base\n",
    "    - `llm`: this takes the prompt from `build_prompt` and retuns the answer\n",
    "    - and finally, we can create a function that calls all the step functions before it called `rag` that takes in a query and produces an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca8f64ce-981a-4d52-bf6f-e1a14ca0cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, num_results = 5):\n",
    "    \n",
    "    boost = {\n",
    "    \"question\":3,\n",
    "    \"text\":1,\n",
    "    \"section\":0.5\n",
    "    }\n",
    "\n",
    "    results = index.search(\n",
    "        query = query,\n",
    "        filter_dict = {\"course\":\"data-engineering-zoomcamp\"},\n",
    "        boost_dict = boost,\n",
    "        num_results = num_results\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f3b1ce68-255f-44fa-987a-1a3764243727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "\n",
    "    # create a context string\n",
    "    context = \"\"\n",
    "\n",
    "    # iterate through the search_results and add results to the context string\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "\n",
    "    # add a user prompt\n",
    "    prompt_template = \"\"\"\n",
    "    You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    CONTEXT:\n",
    "    {context}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()    \n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fcb547fc-b5f8-4941-affd-c7974f8a72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o\",\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "89833a15-3bd9-4d1d-a3ee-b666548c24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_minsearch(question):\n",
    "    results = search(question, 5)\n",
    "    # results = search_elastic(question)\n",
    "    prompt = build_prompt(question, results)\n",
    "    answer = llm(prompt)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a4268-1dc7-45de-a5f9-5fc0e029f4d5",
   "metadata": {},
   "source": [
    "# 1.6 Search with Elasticsearch\n",
    "\n",
    "- Now it's time to replace the minsearch implementation with elasticsearch\n",
    "    - The reason `minsearch` was implemented in the first place was so that in the future if we deploy our RAG in an environment that can't host elasticsearch, we can use something small that runs in memory\n",
    "- To deploy elasticsearch, I simply just added a docker run command to a bash script called `run_elastic.sh` and made sure user had execute access (the original command had -it and --rm as parameters and I'm not sure why):\n",
    "    - ```shell\n",
    "      docker run \\\n",
    "    --name elasticsearch \\\n",
    "    -p 9200:9200 \\\n",
    "    -p 9300:9300 \\\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
    "  ```\n",
    "- you can check if your elastic instance is running by sending a curl request to it: `curl localhost:9200`\n",
    "- So now let's index our documents using Elasticsearch instead of minsearch. This happens in a few steps:\n",
    "    1. Create your es client using `Elasticsearch(url)`\n",
    "    2. Create your indices based on the fields of the documents you want indexed using `es_client.indices.create(index_name, index_settings)`\n",
    "    3. Index your document by iterating through each doc in document and adding them to the index using `es_client.index(index_name, body=[doc for doc in documents])`\n",
    "- Once we have our documents indexed, we can create a search_query\n",
    "- Next, we need to actually search our index using our search_query:\n",
    "    - To do this, we use `es_client.search(index_name, body=search_query)` method by passing our index_name and the search_query we constructed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3cc6fa4-e231-4c9b-aa8d-195e2123f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc53d4b3-d9f6-4600-9911-adca4048ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "949a6ca7-48e8-4bf3-8bc1-c6b420c1271f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'course-questions-homework'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            #\"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "#index_name = \"course-questions\"\n",
    "index_name = \"course-questions-homework\"\n",
    "\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a64ae477-7dc9-4fe7-8fd7-ff2564b84ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9ed443d5-963d-42f6-b612-a9cc70ec3113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 948/948 [00:24<00:00, 38.50it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, body=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a1dd6a4c-ee30-4b78-968a-9325a2db321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = {\n",
    "        \"size\": 3,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": question,\n",
    "                        #\"fields\": [\"question^3\", \"text\", \"section^0.5\"],\n",
    "                        \"fields\": [\"question^4\", \"text\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                 \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"machine-learning-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "65eea6b6-e4fe-47bb-9dfb-5ff61f672134",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do I copy files from a different folder into docker container’s working directory?'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_client.search(index=index_name, body=search_query)['hits']['hits'][2]['_source']['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "80677ebd-ded1-4a9c-a790-f74536191125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_elastic(question):\n",
    "\n",
    "    # create your search query\n",
    "    search_query = {\n",
    "        \"size\": 3,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": question,\n",
    "                        #\"fields\": [\"question^3\", \"text\", \"section^0.5\"],\n",
    "                        \"fields\": [\"question^4\", \"text\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"machine-learning-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # pass the search query to the elasticsearch client\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "\n",
    "    # iterate through the response and pull out the list with the results\n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "615f9961-8d51-42d6-99f7-e8fd85846249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do I execute a command in a running docker container?'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6b4c6f33-2359-41db-885f-bff8b5352981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_elastic(question):\n",
    "    #results = search(question, 5)\n",
    "    results = search_elastic(question)\n",
    "    prompt = build_prompt(question, results)\n",
    "    answer = llm(prompt)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e04576b7-6661-46f5-a5e0-808f6fdc91b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'You can copy files from your local machine into a Docker container using the docker cp command. Here\\'s how to do it:\\nIn the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\\nCOPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGopakumar Gopinathan', 'section': '5. Deploying Machine Learning Models', 'question': 'How do I copy files from a different folder into docker container’s working directory?', 'course': 'machine-learning-zoomcamp'}\n"
     ]
    }
   ],
   "source": [
    "results1 = search_elastic(question)\n",
    "print(results1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2a161799-7cb1-44b8-a972-b1c27a472854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1482"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1 = build_prompt(question, results1)\n",
    "len(prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4016acb6-13b1-413a-b658-947e7447f005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To execute a command in a running Docker container, you can use the `docker exec` command. Here's how you can do it:\n",
      "\n",
      "1. First, you need to identify the container ID or name of the running container. You can do this by listing all running containers using the `docker ps` command.\n",
      "\n",
      "```sh\n",
      "docker ps\n",
      "```\n",
      "\n",
      "This will give you an output similar to this, where you can see the container ID and name:\n",
      "\n",
      "```sh\n",
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n",
      "4a432e93f871        myimage             \"python app.py\"     5 minutes ago       Up 5 minute                              mystifying_babbage\n",
      "```\n",
      "\n",
      "2. Once you have the container ID or name, you can execute a command in the specific container using `docker exec`. For example, to start a bash session in the container, you would use:\n",
      "\n",
      "```sh\n",
      "docker exec -it <container_id_or_name> bash\n",
      "```\n",
      "\n",
      "Replace `<container_id_or_name>` with the actual container ID or name obtained from the `docker ps` command. For example:\n",
      "\n",
      "```sh\n",
      "docker exec -it 4a432e93f871 bash\n",
      "```\n",
      "\n",
      "3. After running this command, you will be inside the running container and can execute commands interactively.\n",
      "\n",
      "Alternatively, if you want to execute a specific command without opening an interactive shell, you can do so directly. For example, to list the contents of a directory inside the container, you might use:\n",
      "\n",
      "```sh\n",
      "docker exec -it <container_id_or_name> ls /path/in/container\n",
      "```\n",
      "\n",
      "For instance:\n",
      "\n",
      "```sh\n",
      "docker exec -it 4a432e93f871 ls /app\n",
      "```\n",
      "\n",
      "This will list the contents of the `/app` directory inside the running container.\n"
     ]
    }
   ],
   "source": [
    "answer1 = llm(prompt1)\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a083f37f-65f0-48a6-9faa-326ec09c745d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1617"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b1e529a1-7984-4a7f-b17f-be65805a37b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can still register for the course after the start date. You won’t be able to submit some of the homework assignments, but you can still participate in the course. To be eligible for a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ projects by the deadline.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cbe5215f-d087-443c-b981-827d2edf025c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /home/codespace/.local/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.5.15 tiktoken-0.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "555625fc-6e4a-42e2-b367-b2154765f2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the prompt: 682\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "myprompt = prompt1\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "tokens = encoding.encode(myprompt)\n",
    "\n",
    "# Count the tokens\n",
    "number_of_tokens = len(tokens)\n",
    "\n",
    "print(f\"Number of tokens in the prompt: {number_of_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c6af661f-e99b-470c-8398-28296bd8d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt2(query, search_results):\n",
    "\n",
    "    # create a context string\n",
    "    context = \"\"\n",
    "\n",
    "    # iterate through the search_results and add results to the context string\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "\n",
    "    # add a user prompt\n",
    "    prompt_template = \"\"\"\n",
    "    Q: {question}\n",
    "    A: {text}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()    \n",
    "\n",
    "    return prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
